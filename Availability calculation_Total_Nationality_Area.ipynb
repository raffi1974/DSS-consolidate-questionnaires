{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8951d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "MAIN_DATA_FILE = 'C:/Users/511232/Desktop/DSS/MERGING GOOGLESHEETS QUESTIONNAIRES/codes/arabic_questionnaires.xlsx'\n",
    "CRITERIA_FILE = 'C:/Users/511232/Desktop/criterias.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ee798ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_criteria_dict(criteria_df, key_language='arabic'):\n",
    "    \"\"\"\n",
    "    Creates a dictionary mapping indicator names to their availability criteria.\n",
    "\n",
    "    Args:\n",
    "        criteria_df (pd.DataFrame): DataFrame containing indicator names and criteria.\n",
    "        key_language (str): 'english' or 'arabic'. Determines which indicator name to use as the key.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping indicator names to their integer criteria.\n",
    "    \"\"\"\n",
    "    if key_language.lower() == 'english':\n",
    "        key_col = 'Indicator_En'\n",
    "    elif key_language.lower() == 'arabic':\n",
    "        key_col = 'Indicator_Ar'\n",
    "    else:\n",
    "        raise ValueError(\"key_language must be 'english' or 'arabic'\")\n",
    "\n",
    "    # Drop rows where the key column is NaN to avoid issues\n",
    "    criteria_df.dropna(subset=[key_col], inplace=True)\n",
    "    \n",
    "    return pd.Series(criteria_df.criteria.values, index=criteria_df[key_col]).to_dict()\n",
    "\n",
    "\n",
    "def calculate_availability(df, group_cols, criteria_dict, global_max_year, year_col='السنة', indicator_col='المؤشر', window_size=5):\n",
    "    \"\"\"\n",
    "    Calculates availability for each group, returning a collapsed Series (one result per group).\n",
    "    This function checks if data points for an indicator are present consistently across defined time windows.\n",
    "    \n",
    "    Example of the process for a single indicator group:\n",
    "    1. indicator_name = group.name[...]\n",
    "       This line just gets the name of the indicator we are working on.\n",
    "       indicator_name = \"Literacy rate\"\n",
    "\n",
    "    2. criteria = criteria_dict.get(indicator_name, 1)\n",
    "       This looks up the \"Literacy rate\" in our criteria dictionary and finds its requirement.\n",
    "       criteria = 2 (meaning we need at least 2 data points per 5-year window)\n",
    "\n",
    "    3. binned_years = pd.cut(...)\n",
    "       This is the categorization step. It takes our list of years and puts each one into a 5-year \"bucket\".\n",
    "       2011 -> [2010, 2015), 2012 -> [2010, 2015)\n",
    "       2016 -> [2015, 2020), 2018 -> [2015, 2020)\n",
    "       2021 -> [2020, 2025), 2022 -> [2020, 2025), 2023 -> [2020, 2025)\n",
    "\n",
    "    4. window_counts = binned_years.value_counts()\n",
    "       This step counts how many data points landed in each bucket.\n",
    "       [2010, 2015): 2\n",
    "       [2015, 2020): 2\n",
    "       [2020, 2025): 3\n",
    "\n",
    "    5. windows_with_sufficient_data = window_counts[window_counts >= criteria]\n",
    "       This is a filter. It keeps only buckets where the count meets our criteria (>= 2).\n",
    "       [2010, 2015): Kept (because 2 >= 2)\n",
    "       [2015, 2020): Kept (because 2 >= 2)\n",
    "       [2020, 2025): Kept (because 3 >= 2)\n",
    "\n",
    "    6. sufficient_windows_set = set(windows_with_sufficient_data.index)\n",
    "       This creates a clean, unique list of the windows that passed the filter.\n",
    "       sufficient_windows_set = { [2010, 2015), [2015, 2020), [2020, 2025) }\n",
    "\n",
    "    7. return 1 if len(...) == len(...) else 0\n",
    "       The final check compares the set of windows with sufficient data against the set of ALL possible windows in our universal time range.\n",
    "       If they match perfectly, it means the indicator is fully available (returns 1), otherwise it's not (returns 0).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.Series(dtype=int)\n",
    "\n",
    "    # Determine the overall year range and create standard bins using the global max year.\n",
    "    min_year = 2010\n",
    "    bins = range(min_year, global_max_year + window_size + 1, window_size)\n",
    "    \n",
    "    # Create a set of all possible windows (bins) that could exist based on the global range.\n",
    "    all_possible_windows = set(pd.cut(pd.Series(range(min_year, global_max_year + 1)), bins=bins, right=False).dropna().unique())\n",
    "    \n",
    "    results = {}\n",
    "    grouped = df.groupby(group_cols)\n",
    "    indicator_col_index = group_cols.index(indicator_col)\n",
    "\n",
    "    for name, group in grouped:\n",
    "        # 'name' is a tuple of the group keys, e.g., ('Indicator A', 'Country X')\n",
    "        indicator_name = name[indicator_col_index]\n",
    "        criteria = criteria_dict.get(indicator_name, 1)\n",
    "\n",
    "        binned_years = pd.cut(group[year_col], bins=bins, right=False)\n",
    "        window_counts = binned_years.value_counts()\n",
    "        \n",
    "        windows_with_sufficient_data = window_counts[window_counts >= criteria]\n",
    "        sufficient_windows_set = set(windows_with_sufficient_data.index)\n",
    "        \n",
    "        results[name] = 1 if len(sufficient_windows_set) == len(all_possible_windows) else 0\n",
    "\n",
    "    return pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4dd5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read successfully.\n",
      "Cleaning source data...\n",
      "Data cleaning complete.\n",
      "Criteria dictionary created with 8 entries.\n",
      "Data maximum year is 2024. This will be used for availability calculation.\n",
      "\n",
      "--- Step 1: Calculating Availability Scores ---\n",
      "\n",
      "--- Step 2: Creating main_availability.xlsx ---\n",
      "Successfully saved 'main_availability.xlsx'\n",
      "\n",
      "--- Step 3: Creating masterfile_detailed_availability.xlsx ---\n",
      "Found 6 unique categorical combinations for the grid.\n",
      "Grid will be built up to year 2025.\n",
      "Created a complete grid with 96 rows.\n",
      "Master file shape after merging data: (106, 9)\n",
      "Successfully saved 'masterfile_detailed_availability.xlsx'\n",
      "\n",
      "--- Step 4: Generating Aggregated Reports ---\n",
      "Saved indicator-country level availability to 'indicator_country_availability.xlsx'\n",
      "Saved country level availability percentages to 'country_availability.xlsx'\n",
      "\n",
      "--- Regional Availability Summary ---\n",
      "Percentage of available indicators for the whole region:\n",
      "Availability total             100.0\n",
      "Availability by nationality      0.0\n",
      "Availability by area            50.0\n",
      "dtype: float64\n",
      "\n",
      "Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire analysis pipeline.\n",
    "    \"\"\"\n",
    "    # 1. Read and Clean Data\n",
    "    try:\n",
    "        main_df = pd.read_excel(MAIN_DATA_FILE)\n",
    "        criteria_df = pd.read_excel(CRITERIA_FILE)\n",
    "        print(\"Files read successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error reading files: {e}. Make sure they are in the correct directory.\")\n",
    "        return\n",
    "\n",
    "    print(\"Cleaning source data...\")\n",
    "    main_df = main_df.loc[:, ~main_df.columns.str.startswith('Unnamed')]\n",
    "    for col in main_df.select_dtypes(include=['object']).columns:\n",
    "        main_df[col] = main_df[col].str.strip()\n",
    "    print(\"Data cleaning complete.\")\n",
    "\n",
    "    # 2. Prepare for Calculations\n",
    "    criteria_dict_ar = create_criteria_dict(criteria_df, key_language='arabic')\n",
    "    print(f\"Criteria dictionary created with {len(criteria_dict_ar)} entries.\")\n",
    "    \n",
    "    # Use the actual max year from the data for the availability calculation\n",
    "    data_max_year = main_df['السنة'].max()\n",
    "    print(f\"Data maximum year is {data_max_year}. This will be used for availability calculation.\")\n",
    "\n",
    "    # 3. Calculate Collapsed Availability Scores\n",
    "    print(\"\\n--- Step 1: Calculating Availability Scores ---\")\n",
    "    \n",
    "    general_availability = calculate_availability(\n",
    "        main_df[main_df['العدد'].notna()], ['المؤشر', 'الدولة'], criteria_dict_ar, data_max_year\n",
    "    )\n",
    "    \n",
    "    nationality_availability = calculate_availability(\n",
    "        main_df[main_df['العدد'].notna() & main_df['المواطنة'].notna() & ~main_df['المواطنة'].isin(['Not applicable', 'غير مطابق', 'Total'])],\n",
    "        ['المؤشر', 'الدولة'], criteria_dict_ar, data_max_year\n",
    "    )\n",
    "\n",
    "    if 'المنطقة' in main_df.columns:\n",
    "        area_availability = calculate_availability(\n",
    "            main_df[main_df['العدد'].notna() & main_df['المنطقة'].notna() & ~main_df['المنطقة'].isin(['Not applicable', 'غير مطابق', 'Total'])],\n",
    "            ['المؤشر', 'الدولة'], criteria_dict_ar, data_max_year\n",
    "        )\n",
    "    else:\n",
    "        area_availability = pd.Series(dtype=int)\n",
    "\n",
    "    # 4. Create a clean DataFrame of Indicator-Country scores\n",
    "    indicator_country_scores = pd.DataFrame({\n",
    "        'Availability total': general_availability,\n",
    "        'Availability by nationality': nationality_availability,\n",
    "        'Availability by area': area_availability\n",
    "    }).reset_index()\n",
    "    indicator_country_scores.rename(columns={'level_0': 'المؤشر', 'level_1': 'الدولة'}, inplace=True)\n",
    "    indicator_country_scores.fillna(0, inplace=True)\n",
    "    \n",
    "    # 5. Create main_availability.xlsx (Original data + availability scores)\n",
    "    print(\"\\n--- Step 2: Creating main_availability.xlsx ---\")\n",
    "    \n",
    "    main_availability_df = pd.merge(main_df, indicator_country_scores, on=['المؤشر', 'الدولة'], how='left')\n",
    "    \n",
    "    # Define and select final columns for output\n",
    "    final_cols = ['المؤشر', 'الدولة', 'السنة', 'العدد', 'المواطنة', 'المنطقة', \n",
    "                  'Availability total', 'Availability by nationality', 'Availability by area']\n",
    "    # Ensure optional columns exist before selecting\n",
    "    cols_to_keep = [col for col in final_cols if col in main_availability_df.columns]\n",
    "    main_availability_df = main_availability_df[cols_to_keep]\n",
    "    \n",
    "    main_availability_df.to_excel('main_availability.xlsx', index=False, engine='openpyxl')\n",
    "    print(\"Successfully saved 'main_availability.xlsx'\")\n",
    "\n",
    "    # 6. Create masterfile_detailed_availability.xlsx (Heatmap-ready file)\n",
    "    print(\"\\n--- Step 3: Creating masterfile_detailed_availability.xlsx ---\")\n",
    "\n",
    "    \"\"\" \n",
    "    This section builds a complete grid for every combination of categories and years.\n",
    "    This is essential for visualizations like heatmaps that need a value for every cell.\n",
    "    \n",
    "    Example of the process:\n",
    "    1.  source_df (Input Data):\n",
    "        المؤشر \t        الدولة \tالسنة \t العدد\n",
    "        النمو السكاني \t مصر   2020 \t 2.5\n",
    "        النمو السكاني \t تونس \t2021 \t 1.1\n",
    "        البطالة         مصر   2021 \t 7.5\n",
    "\n",
    "    2.  unique_combinations:\n",
    "        A DataFrame containing every unique combination of Indicator, Country, Area, and Nationality\n",
    "        that exists in the original data is created. This forms the base for our grid.\n",
    "\n",
    "    3.  A full range of years is created as a separate DataFrame.\n",
    "\n",
    "    4.  Cross Join:\n",
    "        A 'cross' merge is performed between the unique combinations and the years. This creates the\n",
    "        \"Cartesian product\" - a new DataFrame with a row for every combination for every year.\n",
    "\n",
    "    5.  Final Merge:\n",
    "        The data from 'main_availability.xlsx' (which includes 'العدد' and availability scores) is\n",
    "        merged onto this complete grid. This fills in the data for existing points and leaves\n",
    "        NaN for the new year rows we've created.\n",
    "\n",
    "    6.  Filling Scores:\n",
    "        The availability scores for the new empty rows are filled in using forward and backward fill\n",
    "        to ensure consistency.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the columns that make up a unique entity in your data\n",
    "    categorical_cols = ['المؤشر', 'الدولة']\n",
    "    if 'المنطقة' in main_df.columns: categorical_cols.append('المنطقة')\n",
    "    if 'المواطنة' in main_df.columns: categorical_cols.append('المواطنة')\n",
    "\n",
    "    # Get all unique combinations of these categories from the main data\n",
    "    # Drop rows where any of these key identifiers are missing\n",
    "    unique_combinations = main_df[categorical_cols].drop_duplicates().dropna()\n",
    "    print(f\"Found {len(unique_combinations)} unique categorical combinations for the grid.\")\n",
    "\n",
    "    # Determine the max year for the grid. Use the data's max year or 2025, whichever is greater.\n",
    "    grid_max_year = max(data_max_year, 2025)\n",
    "    print(f\"Grid will be built up to year {grid_max_year}.\")\n",
    "    \n",
    "    # Create a DataFrame with a full range of years for the grid\n",
    "    min_year = 2010\n",
    "    all_years = pd.DataFrame({'السنة': range(min_year, grid_max_year + 1)})\n",
    "\n",
    "    # Create the complete grid via a cross join\n",
    "    complete_grid = unique_combinations.merge(all_years, how='cross')\n",
    "    print(f\"Created a complete grid with {len(complete_grid)} rows.\")\n",
    "    \n",
    "    # Merge the actual data (from main_availability_df) onto the complete grid\n",
    "    # The merge keys are all the categorical columns plus the year\n",
    "    merge_keys = categorical_cols + ['السنة']\n",
    "    masterfile_df = pd.merge(complete_grid, main_availability_df, on=merge_keys, how='left')\n",
    "    print(f\"Master file shape after merging data: {masterfile_df.shape}\")\n",
    "\n",
    "    # Forward-fill and back-fill the availability scores to populate the empty year rows\n",
    "    availability_cols = ['Availability total', 'Availability by nationality', 'Availability by area']\n",
    "    availability_cols_exist = [col for col in availability_cols if col in masterfile_df.columns]\n",
    "    \n",
    "    if availability_cols_exist:\n",
    "        masterfile_df[availability_cols_exist] = masterfile_df.groupby(['المؤشر', 'الدولة'])[availability_cols_exist].ffill().bfill()\n",
    "        masterfile_df[availability_cols_exist] = masterfile_df[availability_cols_exist].fillna(0).astype(int)\n",
    "\n",
    "    masterfile_df = masterfile_df[cols_to_keep] # Use the same column selection as before\n",
    "    masterfile_df.to_excel('masterfile_detailed_availability.xlsx', index=False, engine='openpyxl')\n",
    "    print(\"Successfully saved 'masterfile_detailed_availability.xlsx'\")\n",
    "\n",
    "    # 7. Perform and save aggregations\n",
    "    print(\"\\n--- Step 4: Generating Aggregated Reports ---\")\n",
    "    indicator_country_availability_df = indicator_country_scores.copy() # Use the clean scores df\n",
    "    \n",
    "    output_indicator_country = 'indicator_country_availability.xlsx'\n",
    "    indicator_country_availability_df.to_excel(output_indicator_country, index=False, engine='openpyxl')\n",
    "    print(f\"Saved indicator-country level availability to '{output_indicator_country}'\")\n",
    "\n",
    "    agg_cols = [col for col in availability_cols if col in indicator_country_availability_df.columns]\n",
    "    country_agg = indicator_country_availability_df.groupby('الدولة')[agg_cols].sum()\n",
    "    total_indicators = main_df['المؤشر'].nunique()\n",
    "    if total_indicators > 0:\n",
    "        country_availability_pct = (country_agg / total_indicators) * 100\n",
    "    else:\n",
    "        country_availability_pct = country_agg\n",
    "    country_availability_pct.rename(columns=lambda c: c + '_pct', inplace=True)\n",
    "    country_availability_pct.reset_index().to_excel('country_availability.xlsx', index=False, engine='openpyxl')\n",
    "    print(\"Saved country level availability percentages to 'country_availability.xlsx'\")\n",
    "\n",
    "    print(\"\\n--- Regional Availability Summary ---\")\n",
    "    if not indicator_country_availability_df.empty:\n",
    "        regional_sums = indicator_country_availability_df[agg_cols].sum()\n",
    "        regional_pct = (regional_sums / len(indicator_country_availability_df)) * 100\n",
    "        print(\"Percentage of available indicators for the whole region:\")\n",
    "        print(regional_pct)\n",
    "\n",
    "    # 8. Generate heatmap-specific aggregated files from the masterfile\n",
    "    print(\"\\n--- Step 5: Generating Heatmap-Specific Aggregated Files ---\")\n",
    "    \n",
    "    # File 1: Total availability per indicator, country, and year\n",
    "    heatmap_total_df = masterfile_df.groupby(['المؤشر', 'الدولة', 'السنة'])['Availability total'].max().reset_index()\n",
    "    heatmap_total_df.to_excel('heatmap_total.xlsx', index=False, engine='openpyxl')\n",
    "    print(\"Successfully saved 'heatmap_total.xlsx'\")\n",
    "\n",
    "    # File 2: Nationality availability\n",
    "    if 'المواطنة' in masterfile_df.columns and 'Availability by nationality' in masterfile_df.columns:\n",
    "        heatmap_nationality_df = masterfile_df.groupby(['المؤشر', 'الدولة', 'المواطنة', 'السنة'])['Availability by nationality'].max().reset_index()\n",
    "        heatmap_nationality_df.to_excel('heatmap_nationality.xlsx', index=False, engine='openpyxl')\n",
    "        print(\"Successfully saved 'heatmap_nationality.xlsx'\")\n",
    "    else:\n",
    "        print(\"Skipping 'heatmap_nationality.xlsx' due to missing columns.\")\n",
    "\n",
    "    # File 3: Area availability\n",
    "    if 'المنطقة' in masterfile_df.columns and 'Availability by area' in masterfile_df.columns:\n",
    "        heatmap_area_df = masterfile_df.groupby(['المؤشر', 'الدولة', 'المنطقة', 'السنة'])['Availability by area'].max().reset_index()\n",
    "        heatmap_area_df.to_excel('heatmap_area.xlsx', index=False, engine='openpyxl')\n",
    "        print(\"Successfully saved 'heatmap_area.xlsx'\")\n",
    "    else:\n",
    "        print(\"Skipping 'heatmap_area.xlsx' due to missing columns.\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad1813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ecee6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad62e766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a67ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fec5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8615c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d18d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f02bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc37e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8f768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0377c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d4300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba7e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cd15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f040e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5fb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41fa976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a083d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
